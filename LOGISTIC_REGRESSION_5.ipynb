{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "THEORY QUESTION"
      ],
      "metadata": {
        "id": "xQUzemgQkudI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "Logistic Regression is a statistical method used for binary classification problems, where the outcome variable is categorical and typically takes one of two values (e.g., Yes/No, 0/1, True/False). It estimates the probability that a given input belongs to a particular category by applying the logistic (sigmoid) function to a linear combination of the input variables. The output is always between 0 and 1, representing a probability.\n",
        "\n",
        "Difference between Logistic Regression and Linear Regression:\n",
        "\n",
        "Feature\tLogistic Regression\tLinear Regression\n",
        "Purpose\tUsed for classification problems (binary or multi-class).\tUsed for regression problems to predict continuous outcomes.\n",
        "Output\tProbability value between 0 and 1, interpreted as a class label.\tContinuous numerical value.\n",
        "Function Used\tSigmoid (logistic) function.\tStraight-line equation (linear function).\n",
        "Error Metric\tLog-loss or cross-entropy.\tMean Squared Error (MSE).\n",
        "Decision Boundary\tBased on threshold (commonly 0.5).\tNo concept of threshold â€“ predicts actual value.\n",
        "\n",
        "In summary, while both models use similar mathematical foundations, Logistic Regression is specifically designed for classification, and Linear Regression is suited for predicting numeric values.\n",
        "\n"
      ],
      "metadata": {
        "id": "q4xN6edYkuZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "The Sigmoid function plays a crucial role in Logistic Regression by converting the linear output of the model into a probability value between 0 and 1. The mathematical form of the sigmoid function is:\n",
        "\n",
        "ğœ(ğ‘§)=1/1+ğ‘’âˆ’ğ‘§\n",
        "\n",
        "Where z is the linear combination of input features and their corresponding weights, i.e.,\n",
        "\n",
        "    z = w0 â€‹+  w1x1 + w2X2â€‹+â‹¯+wnxn\n",
        "\n",
        "Role in Logistic Regression:\n",
        "Probability Mapping: The sigmoid function takes the raw output of the linear equation and maps it to a value between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class.\n",
        "\n",
        "Classification: A threshold (commonly 0.5) is applied to the sigmoid output to make the final classification. If the output is â‰¥ 0.5, the input is classified as class 1; otherwise, it is class 0.\n",
        "\n",
        "Smooth Gradient: The sigmoid function is smooth and differentiable, which is helpful for optimizing the model using gradient descent.\n",
        "\n",
        "Summary:\n",
        "Without the sigmoid function, logistic regression would output values that are not constrained to the [0, 1] interval, making it unsuitable for classification. The sigmoid function enables logistic regression to model classification problems effectively by translating outputs into probabilities.\n"
      ],
      "metadata": {
        "id": "rFUp03Fykt5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. Overfitting occurs when the model learns noise and random fluctuations in the training data, resulting in poor performance on unseen data. Regularization discourages the model from assigning excessively large weights to features, thus improving generalization.\n",
        "\n",
        "Mathematical Explanation:\n",
        "The original cost function in Logistic Regression is:\n",
        "\n",
        "J(Î¸)=âˆ’m1i=1âˆ‘m[y(i)log(hÎ¸(x(i)))+(1âˆ’y(i))log(1âˆ’hÎ¸(x(i)))]\n",
        "\n",
        "With regularization, a penalty term is added:\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "  J(Î¸)=CostÂ Function+Î»j=1âˆ‘nâˆ£Î¸jâˆ£\n",
        "\n",
        "This can shrink some coefficients to exactly zero, performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "    J(Î¸)=CostÂ Function+Î»j=1âˆ‘nâ€‹Î¸j2\n",
        "\n",
        "This reduces the magnitude of coefficients but keeps them all non-zero.\n",
        "\n",
        "Here, Î» (lambda) is the regularization parameter that controls the strength of the penalty.\n",
        "\n",
        "Why it is Needed:\n",
        "Prevents Overfitting: Keeps model complexity in check by penalizing large weights.\n",
        "\n",
        "Improves Generalization: Enhances performance on unseen data.\n",
        "\n",
        "Feature Selection (L1): Automatically removes irrelevant features by setting their coefficients to zero.\n",
        "\n",
        "Handles Multicollinearity (L2): Stabilizes coefficient estimates when predictors are correlated.\n",
        "\n",
        "Better Model Interpretability: Smaller, more relevant weights lead to simpler models.\n",
        "\n",
        "Summary:\n",
        "Regularization is essential in logistic regression for building robust, generalizable models. By controlling the size of model parameters through L1 or L2 penalties, it avoids overfitting, improves interpretability, and ensures reliable predictions in real-world applications."
      ],
      "metadata": {
        "id": "UkpA7_5Pkt10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "\n",
        "In classification problems, evaluation metrics are used to measure the performance of a model. They help us understand how well the model is predicting classes, detect weaknesses, and choose the right model for a given task.\n",
        "\n",
        "1. Accuracy\n",
        "Definition: The proportion of correctly predicted observations to the total observations.\n",
        "\n",
        "ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦=ğ‘‡ğ‘ƒ+ğ‘‡ğ‘ğ‘‡ğ‘ƒ+ğ‘‡+ğ¹ğ‘\n",
        "\n",
        " Accuracy=TP+TN+FP+FN/TP+TN\n",
        "\n",
        "Where:\n",
        "\n",
        "TP = True Positives\n",
        "\n",
        "TN = True Negatives\n",
        "\n",
        "FP = False Positives\n",
        "\n",
        "FN = False Negatives\n",
        "\n",
        "Importance: Works well for balanced datasets, but can be misleading when classes are imbalanced.\n",
        "\n",
        "2. Precision\n",
        "Definition: The proportion of correctly predicted positive observations out of all predicted positives.\n",
        "\n",
        "ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ\n",
        "\n",
        "Precision=TP+FP/TP\n",
        "Importance: High precision means fewer false positives; important in scenarios like spam detection, where false positives are costly.\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "Definition: The proportion of correctly predicted positive observations out of all actual positives.\n",
        "\n",
        "ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘\n",
        "\n",
        "\n",
        "Importance: High recall means fewer false negatives; critical in medical diagnosis where missing a positive case is dangerous.\n",
        "\n",
        "4. F1-Score\n",
        "Definition: The harmonic mean of precision and recall.\n",
        "\n",
        "ğ¹1=2Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œNÃ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™F1=2Ã—\n",
        "\n",
        "\n",
        "Importance: Useful when there is class imbalance; balances precision and recall.\n",
        "\n",
        "5. ROC Curve and AUC (Area Under the Curve)\n",
        "\n",
        "Definition: The ROC curve plots the True Positive Rate against the False Positive Rate at various thresholds; AUC measures the area under this curve.\n",
        "Importance: AUC close to 1 indicates excellent model performance; useful for comparing classifiers regardless of threshold.\n",
        "\n",
        "Why They Are Important:\n",
        "Comprehensive Performance View: Different metrics highlight different aspects of performance.\n",
        "\n",
        "Handles Imbalanced Data: Metrics like precision, recall, and F1-score give meaningful insights where accuracy fails.\n",
        "\n",
        "Business Impact Alignment: Choice of metric depends on real-world cost of false positives and false negatives.\n",
        "\n",
        "Model Selection: Helps in comparing and selecting the best model for the given problem.\n",
        "\n",
        "Summary:\n",
        "Using multiple evaluation metrics ensures a balanced understanding of model performance. While accuracy is a good starting point, precision, recall, F1-score, and ROC-AUC are critical for making informed decisions, especially in high-stakes or imbalanced datasets."
      ],
      "metadata": {
        "id": "NIp8e12Iktyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Display first 5 rows\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.iloc[:, :-1]   # all columns except target\n",
        "y = df['target']      # target column\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "First 5 rows of dataset:\n",
        "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
        "0                5.1               3.5                1.4               0.2       0\n",
        "1                4.9               3.0                1.4               0.2       0\n",
        "2                4.7               3.2                1.3               0.2       0\n",
        "3                4.6               3.1                1.5               0.2       0\n",
        "4                5.0               3.6                1.4               0.2       0\n",
        "\n",
        "Model Accuracy: 1.0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TybMSZ1r0FvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER 5 Explanation:\n",
        "\n",
        "Dataset Loading: Used load_iris() from sklearn.datasets and converted it to a Pandas DataFrame.\n",
        "\n",
        "Feature & Target Separation: Independent variables (X) and dependent variable (y) were defined.\n",
        "\n",
        "Data Splitting: Used train_test_split() to split into 80% training and 20% testing data.\n",
        "\n",
        "Model Training: Created a LogisticRegression model and fitted it to the training data.\n",
        "\n",
        "Prediction & Accuracy: Used accuracy_score() to evaluate model performance.\n",
        "\n",
        "Result: The model achieved 100% accuracy on the test set (may vary slightly depending on dataset and split)."
      ],
      "metadata": {
        "id": "MeziNnEWktmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "6.  Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.iloc[:, :-1]   # all columns except target\n",
        "y = df['target']      # target column\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model coefficients and accuracy\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n",
        "print(\"\\nIntercept:\")\n",
        "print(model.intercept_)\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n",
        "\n",
        "Sample output:\n",
        "\n",
        "Model Coefficients:\n",
        "[[ 0.41071782  1.46013641 -2.26014217 -0.99274512]\n",
        " [ 0.36509802 -1.53488648  0.63267919 -1.35757893]\n",
        " [-0.77581584  0.07475007  1.62746298  2.35032405]]\n",
        "\n",
        "Intercept:\n",
        "[  0.26722141  1.14173277 -1.40895418]\n",
        "\n",
        "Model Accuracy: 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "z0poew080fti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer 6  Explanation:\n",
        "\n",
        "Dataset Loading: The Iris dataset is loaded using load_iris() from sklearn.datasets.\n",
        "\n",
        "Data Preparation: Features (X) and target (y) are separated, and data is split into training and testing sets.\n",
        "\n",
        "Model Training with L2 Regularization:\n",
        "\n",
        "penalty='l2' specifies Ridge regularization.\n",
        "\n",
        "C=1.0 controls the strength of regularization (smaller C â†’ stronger penalty).\n",
        "\n",
        "solver='lbfgs' is an optimization algorithm suitable for multinomial logistic regression.\n",
        "\n",
        "Model Coefficients: model.coef_ shows the learned weights for each feature.\n",
        "\n",
        "Accuracy: The accuracy is calculated using accuracy_score().\n",
        "\n",
        "Result: The model achieved 100% accuracy on the test set (may vary slightly depending on data split).\n",
        "\n"
      ],
      "metadata": {
        "id": "3Xz1XzSj03D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "7.  Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.iloc[:, :-1]   # all columns except target\n",
        "y = df['target']      # target column\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "      setosa       1.00      1.00      1.00         10\n",
        "  versicolor       1.00      1.00      1.00         10\n",
        "   virginica       1.00      1.00      1.00         10\n",
        "\n",
        "    accuracy                           1.00         30\n",
        "   macro avg       1.00      1.00      1.00         30\n",
        "weighted avg       1.00      1.00      1.00         30"
      ],
      "metadata": {
        "id": "oq0XTMXW1Lwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer 7 Explanation:\n",
        "\n",
        "Dataset Loading: Used load_iris() from sklearn.datasets and converted to a DataFrame.\n",
        "\n",
        "Data Preparation: Split into training and testing sets (80%-20%).\n",
        "\n",
        "Model Training:\n",
        "\n",
        "multi_class='ovr' specifies One-vs-Rest classification for multiclass problems.\n",
        "\n",
        "solver='lbfgs' is an optimizer that supports OvR strategy.\n",
        "\n",
        "max_iter=200 ensures convergence.\n",
        "\n",
        "Prediction: Used the trained model to predict on the test set.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "classification_report() shows precision, recall, and F1-score for each class.\n",
        "\n",
        "Accuracy here is 100%, indicating perfect classification for this dataset (may vary slightly depending on split).\n",
        "\n"
      ],
      "metadata": {
        "id": "QJOU7_EY1dzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "8. Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.iloc[:, :-1]\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],       # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],            # Type of regularization\n",
        "    'solver': ['liblinear']             # Solver that supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=log_reg,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,            # 5-fold cross-validation\n",
        "                           scoring='accuracy',\n",
        "                           verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best cross-validation accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "Sample output:\n",
        "\n",
        "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
        "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
        "Best Cross-Validation Accuracy: 0.9666666666666668\n",
        "\n"
      ],
      "metadata": {
        "id": "dEQquyPv1q55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer 8  Explanation:\n",
        "\n",
        "Parameter Grid:\n",
        "\n",
        "C controls the strength of regularization (smaller â†’ stronger regularization).\n",
        "\n",
        "penalty specifies the type of regularization (L1 or L2).\n",
        "\n",
        "solver is set to 'liblinear' because it supports both L1 and L2 penalties.\n",
        "\n",
        "GridSearchCV: Performs exhaustive search over the parameter grid using 5-fold cross-validation to find the best combination.\n",
        "\n",
        "Output:\n",
        "\n",
        "Best Parameters: The combination of C and penalty giving the highest validation accuracy.\n",
        "\n",
        "Best Cross-Validation Accuracy: The average accuracy over the folds for the best parameters.\n"
      ],
      "metadata": {
        "id": "MomOBWB318DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "9. Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Define features and target\n",
        "X = df.iloc[:, :-1]\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Logistic Regression without scaling\n",
        "# ---------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---------------------------\n",
        "# Logistic Regression with scaling\n",
        "# ---------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_with_scaling)\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "Accuracy without Scaling: 0.9666666666666667\n",
        "Accuracy with Scaling: 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "tSjWnYtz2KsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer 9 Explanation:\n",
        "\n",
        "Without Scaling: Logistic Regression is directly applied to raw features.\n",
        "\n",
        "With Scaling:\n",
        "\n",
        "Used StandardScaler to standardize features so they have mean = 0 and standard deviation = 1.\n",
        "\n",
        "Scaling ensures all features contribute equally to the model and avoids bias from larger magnitude features.\n",
        "\n",
        "Comparison: Accuracy with scaling is slightly better in this example because the optimization converges faster and more reliably."
      ],
      "metadata": {
        "id": "3k8hHVG92Uxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach youâ€™d take to build a\n",
        "Logistic Regression model â€” including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "\n",
        "To build a Logistic Regression model for predicting customer responses in a highly imbalanced dataset (only 5% positive responses), I would follow these steps:\n",
        "\n",
        "1. Data Understanding and Preprocessing\n",
        "Explore Data: Check for missing values, outliers, and feature distributions.\n",
        "\n",
        "Feature Engineering: Create relevant features (e.g., purchase frequency, recency, total spend, marketing channel engagement).\n",
        "\n",
        "Feature Selection: Remove irrelevant or highly correlated features to prevent multicollinearity.\n",
        "\n",
        "2. Handling Class Imbalance (5% positive class)\n",
        "Since the dataset is heavily imbalanced, the model might predict most cases as negative to achieve high accuracy but fail to identify responders.\n",
        "I would handle imbalance by:\n",
        "\n",
        "Resampling Techniques:\n",
        "\n",
        "Oversampling minority class using SMOTE or Random Oversampling.\n",
        "\n",
        "Undersampling majority class if dataset is large enough.\n",
        "\n",
        "Class Weight Adjustment:\n",
        "\n",
        "In LogisticRegression(), set class_weight='balanced' so the algorithm gives higher penalty for misclassifying minority class samples.\n",
        "\n",
        "3. Feature Scaling\n",
        "Use StandardScaler to standardize features (mean = 0, std = 1), since Logistic Regression is sensitive to feature magnitude.\n",
        "\n",
        "Scaling ensures faster convergence and prevents bias toward larger magnitude features.\n",
        "\n",
        "4. Model Training\n",
        "Start with a baseline Logistic Regression model.\n",
        "\n",
        "Use L2 regularization (Ridge) by default to control overfitting.\n",
        "\n",
        "Train the model on the balanced and scaled dataset.\n",
        "\n",
        "5. Hyperparameter Tuning\n",
        "Use GridSearchCV with parameters:\n",
        "\n",
        "C (inverse of regularization strength) â†’ [0.01, 0.1, 1, 10]\n",
        "\n",
        "Penalty â†’ ['l1', 'l2'] (with solver compatible for both, e.g., 'liblinear')\n",
        "\n",
        "Class_weight â†’ [None, 'balanced']\n",
        "\n",
        "Select parameters that maximize recall/F1-score for the minority class.\n",
        "\n",
        "6. Model Evaluation\n",
        "Since accuracy is misleading in imbalanced datasets, focus on metrics that reflect minority class performance:\n",
        "\n",
        "Confusion Matrix â€“ to visualize TP, FP, FN, TN.\n",
        "\n",
        "Precision â€“ avoid targeting customers who wonâ€™t respond (reduce wasted cost).\n",
        "\n",
        "Recall (Sensitivity) â€“ ensure we capture as many responders as possible.\n",
        "\n",
        "F1-score â€“ balance between precision and recall.\n",
        "\n",
        "ROC-AUC & PR-AUC â€“ measure ability to discriminate between classes.\n",
        "\n",
        "7. Business Considerations\n",
        "Prioritize recall if the business goal is to reach as many responders as possible.\n",
        "\n",
        "If marketing budget is limited, prioritize precision to ensure only likely responders are targeted.\n",
        "\n",
        "Provide probability predictions so the marketing team can decide a threshold that aligns with campaign costs and goals.\n",
        "\n",
        "Summary:\n",
        "By combining scaling, class balancing, hyperparameter tuning, and using appropriate evaluation metrics, we can build a robust Logistic Regression model that effectively identifies customers likely to respond, even with severe class imbalance, ensuring optimal marketing spend and improved campaign success."
      ],
      "metadata": {
        "id": "Dbjf5ApJ2m1J"
      }
    }
  ]
}