{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "1. Definition (4 marks)\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "It represents decisions in the form of a tree-like structure, where:\n",
        "\n",
        "Root Node → Represents the entire dataset and the first splitting feature.\n",
        "\n",
        "Internal Nodes → Represent features/attributes and conditions.\n",
        "\n",
        "Branches → Represent the outcome of a decision.\n",
        "\n",
        "Leaf Nodes → Represent the final class label or prediction.\n",
        "\n",
        "In classification, it predicts a categorical output (e.g., spam/not spam, yes/no).\n",
        "\n",
        "2. How it Works (10 marks)\n",
        "Step 1: Selecting the Best Attribute\n",
        "At each node, the algorithm chooses the best feature to split the data.\n",
        "\n",
        "Selection is based on splitting criteria like:\n",
        "\n",
        "Gini Impurity (CART algorithm)\n",
        "\n",
        "Entropy / Information Gain (ID3, C4.5)\n",
        "\n",
        "Chi-square (for categorical features)\n",
        "\n",
        "Step 2: Splitting the Dataset\n",
        "The chosen feature divides the dataset into subsets where the target classes are more homogeneous.\n",
        "\n",
        "Step 3: Recursion\n",
        "The process repeats recursively on each subset, creating new decision nodes and branches.\n",
        "\n",
        "Step 4: Stopping Criteria\n",
        "Tree growth stops when:\n",
        "\n",
        "All samples in a node belong to the same class.\n",
        "\n",
        "No more features are available.\n",
        "\n",
        "A pre-set depth/leaf size limit is reached (to prevent overfitting).\n",
        "\n",
        "Step 5: Classification\n",
        "A new data point is classified by traversing the tree from root to leaf, following the decisions until a leaf node is reached, which gives the predicted class.\n",
        "\n",
        "3. Example (4 marks)\n",
        "Problem: Classify if a student will pass an exam based on Study Hours and Attendance.\n",
        "\n"
      ],
      "metadata": {
        "id": "C1MbiJyK2o6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Root: Attendance ≥ 75%?\n",
        "    Yes → Study Hours ≥ 3?\n",
        "        Yes → PASS\n",
        "        No  → FAIL\n",
        "    No  → FAIL\n"
      ],
      "metadata": {
        "id": "FTWl4Gnj28n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, each condition is a node decision, and the final prediction is at the leaf.\n",
        "\n",
        "4. Advantages & Disadvantages (2 marks)\n",
        "Advantages:\n",
        "\n",
        "Easy to understand and visualize.\n",
        "\n",
        "Handles both numerical and categorical data.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting.\n",
        "\n",
        "Small changes in data can cause large changes in the tree (high variance)."
      ],
      "metadata": {
        "id": "kYXnUgpH2pVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Gini Impurity & Entropy – Impurity Measures in Decision Trees\n",
        "1. Introduction (3 marks)\n",
        "In a Decision Tree, the choice of the “best” feature for splitting is based on how well it separates the data into pure subsets.\n",
        "Impurity Measures are metrics that quantify how mixed the class labels are in a node.\n",
        "Two common measures:\n",
        "\n",
        "Gini Impurity (used in CART)\n",
        "\n",
        "Entropy (used in ID3, C4.5)\n",
        "\n",
        "2. Gini Impurity (7 marks)\n",
        "Definition:\n",
        "Probability that a randomly chosen sample from a node would be misclassified if it were labeled according to the distribution of classes in that node.\n",
        "\n",
        "Formula:\n",
        "Gini Impurity\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "Gini Impurity=1−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = proportion of samples belonging to class i in the node.\n",
        "\n",
        "𝑛\n",
        "n = number of classes.\n",
        "\n",
        "Properties:\n",
        "Range: 0 (pure) to 0.5 (binary case, completely impure).\n",
        "\n",
        "Lower Gini = purer node.\n",
        "\n",
        "Example:\n",
        "Node contains 4 samples → 3 “Yes” and 1 “No”:\n",
        "\n",
        "𝑝\n",
        "𝑌\n",
        "𝑒\n",
        "𝑠\n",
        "=\n",
        "3\n",
        "/\n",
        "4\n",
        "=\n",
        "0.75\n",
        ",\n",
        "\n",
        "𝑝\n",
        "𝑁\n",
        "𝑜\n",
        "=\n",
        "0.25\n",
        "p\n",
        "Yes\n",
        "​\n",
        " =3/4=0.75, p\n",
        "No\n",
        "​\n",
        " =0.25\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.75\n",
        "2\n",
        "+\n",
        "0.25\n",
        "2\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.5625\n",
        "+\n",
        "0.0625\n",
        ")\n",
        "=\n",
        "0.375\n",
        "Gini=1−(0.75\n",
        "2\n",
        " +0.25\n",
        "2\n",
        " )=1−(0.5625+0.0625)=0.375\n",
        "Smaller Gini after a split means better separation.\n",
        "\n",
        "3. Entropy (7 marks)\n",
        "Definition:\n",
        "Measures disorder or uncertainty in a node.\n",
        "\n",
        "Based on the concept from Information Theory (Shannon).\n",
        "\n",
        "Formula:\n",
        "Entropy\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "Entropy=−i=1∑n​pi​log 2​pi\n",
        "\n",
        "where 𝑝𝑖pi\n",
        "  is the proportion of class i.\n",
        "\n",
        "Properties:\n",
        "Range: 0 (pure) to 1 (binary max impurity).\n",
        "\n",
        "Higher Entropy = more disorder.\n",
        "\n",
        "Example:\n",
        "Using the same 3 “Yes” and 1 “No”:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.75\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.75\n",
        "+\n",
        "0.25\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.25\n",
        ")\n",
        "Entropy=−(0.75log\n",
        "2\n",
        "​\n",
        " 0.75+0.25log\n",
        "2\n",
        "​\n",
        " 0.25)\n",
        "=\n",
        "−\n",
        "(\n",
        "0.75\n",
        "×\n",
        "−\n",
        "0.4150\n",
        "+\n",
        "0.25\n",
        "×\n",
        "−\n",
        "2\n",
        ")\n",
        "=−(0.75×−0.4150+0.25×−2)\n",
        "=\n",
        "0.811\n",
        "=0.811\n",
        "Lower Entropy after a split = better separation.\n",
        "\n",
        "4. Impact on Splits (3 marks)\n",
        "Goal: Select the feature and threshold that maximizes purity (minimizes Gini or Entropy) after the split.\n",
        "\n",
        "Information Gain (IG):\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "Entropy (Parent)\n",
        "−\n",
        "Weighted Entropy (Children)\n",
        "IG=Entropy (Parent)−Weighted Entropy (Children)\n",
        "Higher IG = better split.\n",
        "For Gini, we choose the split that yields the lowest weighted Gini.\n",
        "\n",
        "Key Point: Both measures usually give similar results, but:\n",
        "\n",
        "Gini is faster to compute (no logarithms).\n",
        "\n",
        "Entropy is more theoretically linked to information theory.\n",
        "\n",
        "✅ In short:\n",
        "\n",
        "Gini Impurity and Entropy quantify node impurity.\n",
        "\n",
        "Decision Trees split on features that reduce impurity the most.\n",
        "\n",
        "Lower impurity = more homogeneous nodes = better classification performance."
      ],
      "metadata": {
        "id": "OTWOKZdT2pYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "1. Introduction (3 marks)\n",
        "Decision Trees tend to overfit if grown without restrictions — capturing noise instead of patterns.\n",
        "Pruning controls overfitting by reducing the size of the tree.\n",
        "\n",
        "Two main approaches:\n",
        "\n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "Post-Pruning (Prune After Full Growth)\n",
        "\n",
        "2. Pre-Pruning (Early Stopping) (6 marks)\n",
        "Definition:\n",
        "Stops tree growth before it becomes too complex by applying constraints during construction.\n",
        "\n",
        "Common stopping criteria:\n",
        "\n",
        "Maximum tree depth (max_depth)\n",
        "\n",
        "Minimum samples required to split a node (min_samples_split)\n",
        "\n",
        "Minimum impurity decrease required\n",
        "\n",
        "Maximum number of leaf nodes\n",
        "\n",
        "Example:\n",
        "Stop splitting if a node has fewer than 5 samples.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Time & computation efficient — prevents building unnecessarily large trees.\n",
        "\n",
        "3. Post-Pruning (Reduced Error / Cost Complexity) (6 marks)\n",
        "Definition:\n",
        "First grow the full tree (allowing overfitting), then remove branches that provide little improvement in predictive performance.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Reduced Error Pruning: Remove nodes if validation accuracy doesn’t drop.\n",
        "\n",
        "Cost Complexity Pruning (CCP): Balance between tree size and error using complexity parameter (\n",
        "𝛼\n",
        "α).\n",
        "\n",
        "Example:\n",
        "After full tree growth, remove a branch that only classifies 1–2 samples but increases variance.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Higher accuracy — ensures that only harmful branches are removed after seeing the full data structure.\n",
        "\n",
        "4. Key Differences (5 marks)\n",
        "Aspect\tPre-Pruning\tPost-Pruning\n",
        "Timing\tStops growth during building\tPrunes after full tree is built\n",
        "Computation\tFaster, less memory usage\tSlower, requires full tree first\n",
        "Overfitting control\tMay underfit if stopped too early\tMore accurate control after seeing tree\n",
        "Flexibility\tLimited — stops based on thresholds\tFlexible — can evaluate whole structure\n",
        "\n",
        "✅ In short:\n",
        "\n",
        "Pre-Pruning: Prevents over-complexity before it happens (fast but risk of underfitting).\n",
        "\n",
        "Post-Pruning: Lets tree overfit, then trims excess (slower but often more accurate)."
      ],
      "metadata": {
        "id": "DlvpwzlE2pbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Information Gain in Decision Trees\n",
        "1. Introduction (3 marks)\n",
        "In a Decision Tree, the goal is to choose the feature that best separates the data into pure subsets.\n",
        "Information Gain (IG) is a measure used (especially in ID3 and C4.5 algorithms) to determine how much “information” a feature gives us about the target variable.\n",
        "\n",
        "2. Definition (5 marks)\n",
        "Information Gain = Reduction in uncertainty about the target variable after splitting on a particular feature.\n",
        "\n",
        "It is based on Entropy from information theory.\n",
        "\n",
        "Formula:\n",
        "  \n",
        "    G(S,A)=Entropy(S)−v∈Values(A)∑​∣S∣∣Sv∣×Entropy(Sv)\n",
        "\n",
        "Where:\n",
        "\n",
        "S = dataset at the current node\n",
        "A = feature to split on\n",
        "Sv= subset of S where A=v\n",
        "∣S∣∣Sv∣= proportion of samples in subset Sv\n",
        "\n",
        "3. Step-by-Step Working (6 marks)\n",
        "Step 1: Calculate Entropy of the parent node (before split).\n",
        "Step 2: For each possible value of a feature, split the dataset and calculate the Entropy of each subset.\n",
        "Step 3: Compute the Weighted Average Entropy after the split.\n",
        "Step 4: Subtract this weighted entropy from the parent node’s entropy → this is the Information Gain.\n",
        "Step 5: Repeat for all features, choose the one with highest IG.\n",
        "\n",
        "4. Example (3 marks)\n",
        "Suppose we want to classify “Play Tennis” based on “Weather”:\n",
        "\n",
        "Parent Entropy (before split): 0.94\n",
        "Entropy after splitting on Weather: 0.69\n",
        "\n",
        "IG=0.94−0.69=0.25\n",
        "→ This means “Weather” reduces uncertainty by 0.25 bits of information.\n",
        "\n",
        "5. Importance for Choosing the Best Split (3 marks)\n",
        "High IG means the feature gives a better separation of classes → leads to purer child nodes.\n",
        "\n",
        "Ensures that the most informative features are chosen at higher levels of the tree.\n",
        "\n",
        "Helps improve classification accuracy and reduce tree depth.\n",
        "\n",
        "✅ In short:\n",
        "Information Gain measures how much knowing a feature helps in predicting the target.\n",
        "The Decision Tree selects the feature with the highest IG at each node to maximize purity and efficiency."
      ],
      "metadata": {
        "id": "l7kmCy-Z2peI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Real-World Applications of Decision Trees\n",
        "1. Introduction (2 marks)\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "Its interpretability makes it popular across industries.\n",
        "\n",
        "2. Common Real-World Applications (9 marks)\n",
        "Medical Diagnosis\n",
        "\n",
        "Predicting diseases based on patient symptoms, test results, and medical history.\n",
        "\n",
        "Example: Classifying whether a tumor is malignant or benign.\n",
        "\n",
        "Customer Churn Prediction\n",
        "\n",
        "Telecom or subscription services use trees to predict customers likely to leave.\n",
        "\n",
        "Credit Scoring & Risk Assessment\n",
        "\n",
        "Banks use trees to evaluate loan applications based on income, credit history, etc.\n",
        "\n",
        "Fraud Detection\n",
        "\n",
        "Identify fraudulent transactions in banking or e-commerce.\n",
        "\n",
        "Retail & Marketing\n",
        "\n",
        "Segment customers for targeted advertising campaigns.\n",
        "\n",
        "Manufacturing Quality Control\n",
        "\n",
        "Detect defective products based on production parameters.\n",
        "\n",
        "Weather Prediction\n",
        "\n",
        "Predicting rain, storms, or suitable conditions for farming.\n",
        "\n",
        "Education\n",
        "\n",
        "Predicting student performance or identifying at-risk students.\n",
        "\n",
        "HR Analytics\n",
        "\n",
        "Screening job candidates based on qualifications and past performance.\n",
        "\n",
        "3. Main Advantages (5 marks)\n",
        "Easy to Understand & Interpret – No complex mathematics; resembles human decision-making.\n",
        "\n",
        "Handles Both Numerical & Categorical Data – Versatile in data types.\n",
        "\n",
        "No Need for Feature Scaling – Unlike algorithms such as SVM or KNN.\n",
        "\n",
        "Can Capture Non-Linear Relationships – Flexible in splitting criteria.\n",
        "\n",
        "Works Well for Small to Medium Datasets – Requires less preprocessing.\n",
        "\n",
        "4. Main Limitations (4 marks)\n",
        "Overfitting – Tends to fit noise in the training data if not pruned.\n",
        "\n",
        "Instability – Small changes in data can cause large changes in the tree.\n",
        "\n",
        "Biased Towards Features with More Levels – Categorical features with many categories may dominate.\n",
        "\n",
        "Not Always the Most Accurate – Often outperformed by ensemble methods like Random Forests.\n",
        "\n",
        "✅ In short:\n",
        "Decision Trees are widely used in healthcare, finance, marketing, and more due to their simplicity and interpretability, but they must be pruned or combined with ensembles to avoid overfitting and improve stability."
      ],
      "metadata": {
        "id": "rkX0QPTN2phD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "6. ● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "\n",
        "Python Program – Decision Tree Classifier on Iris Dataset (Gini Criterion)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "\n",
        "# Question 6: Decision Tree Classifier on Iris Dataset using Gini Criterion\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# 2. Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# 4. Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 6. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 7. Get feature importances\n",
        "feature_importances = pd.Series(\n",
        "    clf.feature_importances_,\n",
        "    index=iris.feature_names\n",
        ")\n",
        "\n",
        "# 8. Print results\n",
        "print(\"Decision Tree Classifier (Gini Criterion)\")\n",
        "print(\"========================================\")\n",
        "print(f\"Accuracy on test data: {accuracy:.2f}\")\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "m1jUnmTZ_PWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample output:\n",
        "\n",
        "Decision Tree Classifier (Gini Criterion)\n",
        "========================================\n",
        "Accuracy on test data: 1.00\n",
        "\n",
        "Feature Importances:\n",
        "petal length (cm)    0.57\n",
        "petal width (cm)     0.43\n",
        "sepal width (cm)     0.00\n",
        "sepal length (cm)    0.00\n",
        "dtype: float64\n"
      ],
      "metadata": {
        "id": "h1n7f9ay_mir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "# Question: Compare Decision Tree with max_depth=3 vs Fully-Grown Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree with max_depth=3\n",
        "shallow_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "shallow_tree.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train Fully-Grown Decision Tree (no max_depth limit)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred_shallow = shallow_tree.predict(X_test)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "\n",
        "# 6. Accuracy scores\n",
        "acc_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"Decision Tree Accuracy Comparison\")\n",
        "print(\"=================================\")\n",
        "print(f\"Shallow Tree (max_depth=3) Accuracy: {acc_shallow:.2f}\")\n",
        "print(f\"Fully-Grown Tree Accuracy:          {acc_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "RD-QHPxX_yld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample output:\n",
        "\n",
        "Decision Tree Accuracy Comparison\n",
        "=================================\n",
        "Shallow Tree (max_depth=3) Accuracy: 0.97\n",
        "Fully-Grown Tree Accuracy:          1.00\n"
      ],
      "metadata": {
        "id": "ODtVkPhd_3vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Question 8: Decision Tree Regressor on California Housing Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data       # Features\n",
        "y = housing.target     # Target variable (Median House Value)\n",
        "\n",
        "# 2. Split data into train (80%) and test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 4. Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 6. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# 7. Get feature importances\n",
        "feature_importances = pd.Series(\n",
        "    regressor.feature_importances_,\n",
        "    index=housing.feature_names\n",
        ")\n",
        "\n",
        "# 8. Print results\n",
        "print(\"Decision Tree Regressor on California Housing Dataset\")\n",
        "print(\"=====================================================\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\\n\")\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WHRMeiiYADuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample output:\n",
        "\n",
        "Decision Tree Regressor on California Housing Dataset\n",
        "=====================================================\n",
        "Mean Squared Error (MSE): 0.1615\n",
        "\n",
        "Feature Importances:\n",
        "MedInc      0.5331\n",
        "Latitude    0.1387\n",
        "Longitude   0.1294\n",
        "AveOccup    0.0596\n",
        "HouseAge    0.0541\n",
        "AveRooms    0.0418\n",
        "AveBedrms   0.0275\n",
        "Population  0.0160\n",
        "dtype: float64\n"
      ],
      "metadata": {
        "id": "rtJBwxhQALKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Question 9: Decision Tree Hyperparameter Tuning with GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize base Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# 5. Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt_clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 6. Fit GridSearchCV on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Get the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 8. Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 9. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 10. Print results\n",
        "print(\"Decision Tree Hyperparameter Tuning (Iris Dataset)\")\n",
        "print(\"==================================================\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NX5efyRlAUhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample output:\n",
        "\n",
        "Decision Tree Hyperparameter Tuning (Iris Dataset)\n",
        "==================================================\n",
        "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "Best Cross-Validation Accuracy: 0.9667\n",
        "Test Set Accuracy: 1.0000\n"
      ],
      "metadata": {
        "id": "UF77nt-zAYfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "1. Handle Missing Values\n",
        "Identify missing values using pandas.isnull() or df.info().\n",
        "\n",
        "Numerical features: Replace missing values using mean or median imputation (SimpleImputer(strategy='median') is preferred for skewed data).\n",
        "\n",
        "Categorical features: Replace missing values using most frequent category (SimpleImputer(strategy='most_frequent')).\n",
        "\n",
        "If too many missing values exist in a feature (>40–50%), consider dropping that feature to avoid noise.\n",
        "\n",
        "For advanced handling, use KNN imputation or Iterative Imputer to leverage patterns in other features.\n",
        "\n",
        "2. Encode the Categorical Features (3 marks)\n",
        "Label Encoding for ordinal features (where order matters, e.g., disease stage: mild < moderate < severe).\n",
        "\n",
        "One-Hot Encoding for nominal features (no order, e.g., blood type: A, B, AB, O).\n",
        "\n",
        "Use OneHotEncoder(handle_unknown='ignore') to avoid errors with unseen categories during prediction.\n",
        "\n",
        "Since Decision Trees are not affected by feature scaling, no standardization is needed.\n",
        "\n",
        "3. Train a Decision Tree Model (4 marks)\n",
        "Data Split: Use train_test_split() (e.g., 80% training, 20% testing).\n",
        "\n",
        "Model Choice: Use DecisionTreeClassifier(criterion='gini', random_state=42) or criterion='entropy' depending on preference.\n",
        "\n",
        "Fit the Model: Train on the preprocessed training set.\n",
        "\n",
        "Check Feature Importances: Identify which medical features are most predictive (e.g., blood pressure, lab test results).\n",
        "\n",
        "4. Tune Hyperparameters (4 marks)\n",
        "Use GridSearchCV or RandomizedSearchCV to optimize:\n",
        "\n",
        "max_depth (controls tree depth, prevents overfitting)\n",
        "\n",
        "min_samples_split (minimum samples to split a node)\n",
        "\n",
        "min_samples_leaf (minimum samples in a leaf node)\n",
        "\n",
        "criterion (Gini vs Entropy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_JnJsXpcAqAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Example:\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "Evaluate parameters using cross-validation to ensure robust performance."
      ],
      "metadata": {
        "id": "bIxlft88A0yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate Performance (3 marks)\n",
        "Metrics:\n",
        "\n",
        "Accuracy → overall correctness\n",
        "\n",
        "Precision & Recall → important for imbalanced healthcare data\n",
        "(Recall is critical to minimize false negatives in disease detection)\n",
        "\n",
        "F1-score → balances precision and recall\n",
        "\n",
        "ROC-AUC → measures separability of classes\n",
        "\n",
        "Use confusion matrix to understand type of errors (false positives vs false negatives).\n",
        "\n",
        "If data is imbalanced, use stratified splits and possibly class weights.\n",
        "\n",
        "6. Business Value in Real-World Setting (2 marks)\n",
        "Early Disease Detection: Helps doctors identify at-risk patients earlier, improving treatment outcomes.\n",
        "\n",
        "Personalized Treatment: High-importance features guide targeted interventions (e.g., recommending specific tests).\n",
        "\n",
        "Operational Efficiency: Automates initial screening, reducing workload on medical staff.\n",
        "\n",
        "Cost Reduction: Avoids unnecessary tests for low-risk patients, optimizing healthcare spending.\n",
        "\n",
        "Patient Satisfaction: Faster diagnosis leads to quicker care and better patient trust.\n",
        "\n",
        "✅ In summary:\n",
        "We preprocess the data (handle missing values & encode categories), train and tune a Decision Tree, evaluate using healthcare-relevant metrics, and apply it in a way that delivers better patient outcomes, efficiency, and cost savings."
      ],
      "metadata": {
        "id": "gLJvbMFBA8x6"
      }
    }
  ]
}