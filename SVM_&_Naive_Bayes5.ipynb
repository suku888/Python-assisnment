{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Support Vector Machine (SVM)\n",
        "\n",
        "Definition\n",
        "A Support Vector Machine is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points of different classes with the maximum margin.\n",
        "\n",
        "Key Concepts\n",
        "\n",
        "Hyperplane ‚Äì A decision boundary that separates different classes in feature space.\n",
        "\n",
        "Margin ‚Äì The distance between the hyperplane and the nearest data points from each class.\n",
        "\n",
        "Support Vectors ‚Äì The data points closest to the hyperplane, which directly influence its position and orientation.\n",
        "\n",
        "Linear vs. Non-linear SVM ‚Äì\n",
        "\n",
        "Linear SVM: Works when data is linearly separable.\n",
        "\n",
        "Non-linear SVM: Uses the kernel trick to map data into higher-dimensional space for separation.\n",
        "\n",
        "Kernel Functions ‚Äì Examples: Linear, Polynomial, Radial Basis Function (RBF), Sigmoid.\n",
        "\n",
        "Regularization Parameter (C) ‚Äì Controls the trade-off between maximizing margin and minimizing classification errors.\n",
        "\n",
        "How it Works (Step-by-Step) (8 marks)\n",
        "\n",
        "Input Data: SVM takes labeled training data (features + target classes).\n",
        "\n",
        "Choosing a Hyperplane: It searches for a hyperplane that best separates the classes.\n",
        "\n",
        "Maximizing the Margin: It ensures the chosen hyperplane has the largest possible margin from the nearest points.\n",
        "\n",
        "Support Vectors: Identifies the critical boundary points that define the margin.\n",
        "\n",
        "Kernel Trick (if needed): Transforms data into higher dimensions to make separation possible in non-linear cases.\n",
        "\n",
        "Prediction: For a new data point, SVM determines on which side of the hyperplane it lies to classify it.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Works well for both linear and non-linear data.\n",
        "\n",
        "Effective in high-dimensional spaces.\n",
        "\n",
        "Robust against overfitting in many cases.\n",
        "\n",
        "Limitations\n",
        "\n",
        "Computationally expensive for large datasets.\n",
        "\n",
        "Choice of kernel and parameters is crucial for performance.\n",
        "\n",
        "‚úÖ Final Summary:\n",
        "SVM is a powerful classification algorithm that finds the optimal decision boundary with maximum margin. It uses support vectors and kernel functions to handle both linear and non-linear data, making it widely used in text classification, image recognition, and bioinformatics."
      ],
      "metadata": {
        "id": "TVfKTJvLJbEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Introduction\n",
        "\n",
        "Support Vector Machine (SVM) aims to find a decision boundary (hyperplane) that separates classes. The concept of margin defines the distance between the hyperplane and the nearest data points.\n",
        "Depending on how strictly we separate the data, SVM can be:\n",
        "\n",
        "Hard Margin SVM\n",
        "\n",
        "Soft Margin SVM\n",
        "\n",
        "1. Hard Margin SVM\n",
        "\n",
        "Definition: A hard margin SVM finds the hyperplane that perfectly separates all training points without any misclassification.\n",
        "\n",
        "Assumption: Data must be linearly separable.\n",
        "\n",
        "Properties:\n",
        "\n",
        "No tolerance for misclassification.\n",
        "\n",
        "Maximizes margin strictly.\n",
        "\n",
        "Works well for noise-free datasets.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and gives perfect separation for clean data.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Fails if data is noisy or overlapping.\n",
        "\n",
        "Not suitable for real-world datasets with outliers.\n",
        "\n",
        "2. Soft Margin SVM\n",
        "\n",
        "Definition: A soft margin SVM allows some misclassification by introducing slack variables to handle non-separable data.\n",
        "\n",
        "Assumption: Data may be non-linearly separable or contain noise.\n",
        "\n",
        "Properties:\n",
        "\n",
        "Balances margin maximization and classification errors.\n",
        "\n",
        "Controlled by regularization parameter C:\n",
        "\n",
        "Large C ‚Üí fewer misclassifications (behaves like hard margin).\n",
        "\n",
        "Small C ‚Üí wider margin but more misclassifications allowed.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Works well with noisy data.\n",
        "\n",
        "More robust for real-world applications.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Requires tuning of C for best results.\n",
        "\n",
        "3. Key Differences Table (4 marks)\n",
        "Feature\tHard Margin SVM\tSoft Margin SVM\n",
        "Data Requirement\tPerfectly linearly separable data\tCan handle overlapping/noisy data\n",
        "Misclassification\tNot allowed\tAllowed (controlled by slack variables)\n",
        "Robustness to Noise\tVery low\tHigh\n",
        "Parameter C\tNot required\tRequired to control trade-off\n",
        "Use Case\tIdeal for clean datasets\tSuitable for most real-world datasets\n",
        "Conclusion (1 mark)\n",
        "\n",
        "Hard margin SVM is a strict approach for perfectly separable data, while soft margin SVM is flexible, allowing better performance on noisy, real-world datasets."
      ],
      "metadata": {
        "id": "0o2gM2QoJmyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Definition\n",
        "\n",
        "The Kernel Trick is a mathematical technique in Support Vector Machines (SVM) that allows the algorithm to perform classification in a higher-dimensional space without explicitly computing the coordinates of the data in that space.\n",
        "It uses a kernel function to compute the inner product between two data points in the transformed space directly.\n",
        "\n",
        "Why It‚Äôs Needed\n",
        "\n",
        "Some datasets are not linearly separable in their original feature space.\n",
        "\n",
        "By mapping data into a higher dimension, it can become linearly separable.\n",
        "\n",
        "Directly computing in high dimensions is computationally expensive; the kernel trick avoids that cost.\n",
        "\n",
        "How It Works\n",
        "\n",
        "Original space: Data points cannot be separated by a straight line (hyperplane).\n",
        "\n",
        "Mapping function œÜ(x): Transforms data to a higher-dimensional space where separation is possible.\n",
        "\n",
        "Kernel function K(x·µ¢, x‚±º): Calculates dot products in the higher-dimensional space without explicitly transforming the data.\n",
        "\n",
        "This makes SVM capable of solving non-linear classification problems efficiently.\n",
        "\n",
        "Example ‚Äì Radial Basis Function (RBF) Kernel\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë•\n",
        "ùëó\n",
        ")\n",
        "=\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "ùõæ\n",
        "‚à•\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë•\n",
        "ùëó\n",
        "‚à•\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ",x\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        ")=exp(‚àíŒ≥‚à•x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚àíx\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        "‚à•\n",
        "2\n",
        ")\n",
        "\n",
        "Parameters:\n",
        "\n",
        "Œ≥ (gamma): Controls the influence of each training point.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Measures similarity between two points.\n",
        "\n",
        "Closer points ‚Üí higher similarity value (near 1).\n",
        "\n",
        "Farther points ‚Üí lower similarity value (near 0).\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Handwritten digit recognition: Digits are often non-linearly separable in raw pixel space. The RBF kernel maps them into a higher-dimensional space where SVM can separate them effectively.\n",
        "\n",
        "Advantages of Kernel Trick (2 marks)\n",
        "\n",
        "Handles complex, non-linear relationships.\n",
        "\n",
        "Avoids high computational cost of explicit transformations.\n",
        "\n",
        "‚úÖ Final Summary:\n",
        "The Kernel Trick lets SVM work in higher dimensions without direct computation, enabling efficient non-linear classification. For example, the RBF kernel is widely used in image recognition tasks to separate complex patterns."
      ],
      "metadata": {
        "id": "67k58eUrJm1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "Definition\n",
        "\n",
        "Na√Øve Bayes is a supervised machine learning algorithm based on Bayes‚Äô Theorem. It is mainly used for classification tasks.\n",
        "It predicts the probability that a data point belongs to a particular class based on the prior probability of each class and the likelihood of the features given the class.\n",
        "\n",
        "Bayes‚Äô Theorem formula:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "‚ãÖ\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X)=\n",
        "P(X)\n",
        "P(X‚à£C)‚ãÖP(C)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X) = Posterior probability (probability of class given features)\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(X‚à£C) = Likelihood (probability of features given class)\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "P(C) = Prior probability of class\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(X) = Evidence\n",
        "\n",
        "Why It‚Äôs Called ‚ÄúNa√Øve‚Äù (6 marks)\n",
        "\n",
        "It assumes that all features are independent of each other given the class label.\n",
        "\n",
        "In reality, features in a dataset are often correlated (e.g., in email classification, ‚Äúfree‚Äù and ‚Äúwin‚Äù often appear together).\n",
        "\n",
        "Despite this unrealistic assumption, it often works surprisingly well in practice ‚Äî hence the term ‚Äúna√Øve.‚Äù\n",
        "\n",
        "How It Works\n",
        "\n",
        "Training phase:\n",
        "\n",
        "Calculate the prior probabilities\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "P(C) for each class.\n",
        "\n",
        "Calculate the likelihood\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(X\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£C) for each feature given the class.\n",
        "\n",
        "Prediction phase:\n",
        "\n",
        "Apply Bayes‚Äô Theorem to compute posterior probabilities for each class.\n",
        "\n",
        "Choose the class with the highest posterior probability.\n",
        "\n",
        "Example Use Case\n",
        "\n",
        "Spam Email Detection:\n",
        "\n",
        "Features = presence of certain words (‚Äúoffer‚Äù, ‚Äúfree‚Äù, ‚Äúbuy‚Äù)\n",
        "\n",
        "Classes = ‚ÄúSpam‚Äù or ‚ÄúNot Spam‚Äù\n",
        "\n",
        "Na√Øve Bayes predicts whether an email is spam based on the likelihood of these words in spam vs. non-spam emails.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Fast and works well with large datasets.\n",
        "\n",
        "Effective for text classification problems.\n",
        "\n",
        "Limitations\n",
        "\n",
        "The independence assumption is rarely true in real-world data.\n",
        "\n",
        "Poor performance if features are highly dependent.\n",
        "\n",
        "‚úÖ Final Summary:\n",
        "Na√Øve Bayes is a probability-based classifier that applies Bayes‚Äô Theorem with the na√Øve assumption of feature independence. Despite this assumption, it is widely used and effective for many classification problems like spam filtering and sentiment analysis."
      ],
      "metadata": {
        "id": "yKg35g4zJm5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Na√Øve Bayes Variants\n",
        "\n",
        "Na√Øve Bayes classifiers come in different forms depending on the type of feature distribution assumed. The three most common are:\n",
        "\n",
        "Gaussian Na√Øve Bayes\n",
        "\n",
        "Multinomial Na√Øve Bayes\n",
        "\n",
        "Bernoulli Na√Øve Bayes\n",
        "\n",
        "1. Gaussian Na√Øve Bayes\n",
        "\n",
        "Definition: Assumes that continuous features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Probability formula for feature\n",
        "ùë•\n",
        "x given class\n",
        "ùê∂\n",
        "C:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ùúã\n",
        "ùúé\n",
        "ùê∂\n",
        "2\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "(\n",
        "ùë•\n",
        "‚àí\n",
        "ùúá\n",
        "ùê∂\n",
        ")\n",
        "2\n",
        "2\n",
        "ùúé\n",
        "ùê∂\n",
        "2\n",
        ")\n",
        "P(x‚à£C)=\n",
        "2œÄœÉ\n",
        "C\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "exp(‚àí\n",
        "2œÉ\n",
        "C\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(x‚àíŒº\n",
        "C\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "When to use:\n",
        "\n",
        "For continuous numeric data (e.g., age, height, temperature).\n",
        "\n",
        "Example: Classifying flowers in the Iris dataset based on petal/sepal length.\n",
        "\n",
        "2. Multinomial Na√Øve Bayes\n",
        "\n",
        "Definition: Assumes that features are discrete counts (e.g., frequency of words in text).\n",
        "\n",
        "Works well for:\n",
        "\n",
        "Document classification, text mining, and Natural Language Processing (NLP).\n",
        "\n",
        "When to use:\n",
        "\n",
        "For count-based features (e.g., number of times a word appears in a document).\n",
        "\n",
        "Example: Classifying news articles into topics based on word counts.\n",
        "\n",
        "3. Bernoulli Na√Øve Bayes\n",
        "\n",
        "Definition: Assumes binary features (0 or 1) indicating whether a particular feature is present or absent.\n",
        "\n",
        "When to use:\n",
        "\n",
        "For binary-valued features.\n",
        "\n",
        "Example: Spam detection where each feature indicates whether a specific word appears in an email (1 = present, 0 = absent).\n",
        "\n",
        "Comparison Table\n",
        "Variant\tFeature Type\tTypical Use Case\n",
        "Gaussian\tContinuous numeric\tMedical data, sensor readings, physical measurements\n",
        "Multinomial\tDiscrete counts\tText classification (word frequency)\n",
        "Bernoulli\tBinary (0/1)\tBinary text features, document presence/absence of words\n",
        "\n",
        "‚úÖ Final Summary:\n",
        "\n",
        "Gaussian NB ‚Üí continuous data following normal distribution.\n",
        "\n",
        "Multinomial NB ‚Üí discrete count data (e.g., word frequencies).\n",
        "\n",
        "Bernoulli NB ‚Üí binary features indicating presence or absence.\n",
        "Choosing the correct variant ensures better performance because each assumes a specific data distribution."
      ],
      "metadata": {
        "id": "lB-n-AW7JtqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "6. ‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "Question 6:  Write a Python program to:\n",
        "‚óè Load the Iris dataset\n",
        "‚óè Train an SVM Classifier with a linear kernel\n",
        "‚óè Print the model's accuracy and support vectors.\n",
        "\n",
        "# Question 6: SVM Classifier with Linear Kernel on Iris Dataset\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data   # Features\n",
        "y = iris.target # Target labels\n",
        "\n",
        "# 2. Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Create SVM model with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "\n",
        "# 4. Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 6. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"SVM Classifier with Linear Kernel\")\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Number of Support Vectors for each class: {svm_model.n_support_}\")\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "id": "ebFkX5GpNkUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample output:\n",
        "\n",
        "SVM Classifier with Linear Kernel\n",
        "-----------------------------------\n",
        "Accuracy: 100.00%\n",
        "Number of Support Vectors for each class: [2 3 3]\n",
        "Support Vectors:\n",
        " [[5.1 3.5 1.4 0.2]\n",
        "  [4.9 3.  1.4 0.2]\n",
        "  [6.9 3.1 4.9 1.5]\n",
        "  [5.6 2.9 3.6 1.3]\n",
        "  [6.5 3.  5.8 2.2]\n",
        "  [7.2 3.6 6.1 2.5]]\n"
      ],
      "metadata": {
        "id": "Grjw8UdKNwBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset\n",
        "‚óè Train a Gaussian Na√Øve Bayes model\n",
        "‚óè Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "# Question: Gaussian Na√Øve Bayes on Breast Cancer Dataset\n",
        "\n",
        "# 1. Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 2. Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data   # Features\n",
        "y = breast_cancer.target # Target labels\n",
        "\n",
        "# 3. Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Create Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 5. Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 6. Make predictions on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 7. Print classification report\n",
        "print(\"Gaussian Na√Øve Bayes - Breast Cancer Dataset\")\n",
        "print(\"---------------------------------------------\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "id": "IOToI5QcOMR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample Output (Will Vary Slightly)\n",
        "\n",
        "Gaussian Na√Øve Bayes - Breast Cancer Dataset\n",
        "---------------------------------------------\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "   malignant       0.96      0.91      0.94        42\n",
        "      benign       0.94      0.98      0.96        72\n",
        "\n",
        "    accuracy                           0.95       114\n",
        "   macro avg       0.95      0.95      0.95       114\n",
        "weighted avg       0.95      0.95      0.95       114\n"
      ],
      "metadata": {
        "id": "H1xmUrA6OTsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8: Write a Python program to:\n",
        "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "‚óè Print the best hyperparameters and accuracy.\n",
        "\n",
        "# Question 8: SVM Classifier with GridSearchCV on Wine Dataset\n",
        "\n",
        "# 1. Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data   # Features\n",
        "y = wine.target # Labels\n",
        "\n",
        "# 3. Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],       # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001], # Kernel coefficient\n",
        "    'kernel': ['rbf']             # Using RBF kernel\n",
        "}\n",
        "\n",
        "# 5. Create SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# 6. Apply GridSearchCV (5-fold cross-validation)\n",
        "grid_search = GridSearchCV(svm_model, param_grid, refit=True, verbose=0, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 8. Make predictions with the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 9. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 10. Print results\n",
        "print(\"SVM with GridSearchCV - Wine Dataset\")\n",
        "print(\"--------------------------------------\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TOIXaw3eOilk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample Output (Will Vary)\n",
        "\n",
        "SVM with GridSearchCV - Wine Dataset\n",
        "--------------------------------------\n",
        "Best Hyperparameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
        "Accuracy: 100.00%\n"
      ],
      "metadata": {
        "id": "gw4AoO3zOr2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9: Write a Python program to:\n",
        "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "‚óè Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "# Question 9: Na√Øve Bayes on Synthetic Text Dataset with ROC-AUC Score\n",
        "\n",
        "# 1. Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['comp.graphics', 'sci.space']  # Two classes for binary classification\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = newsgroups.data   # Text data\n",
        "y = newsgroups.target # Labels (0 or 1)\n",
        "\n",
        "# 3. Convert text to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 4. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Create and train Multinomial Na√Øve Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict probabilities for ROC-AUC\n",
        "y_proba = nb_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "# 7. Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# 8. Print results\n",
        "print(\"Na√Øve Bayes on Synthetic Text Dataset (20 Newsgroups)\")\n",
        "print(\"----------------------------------------------------\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "NuswsrdFPC3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample Output (Will Vary)\n",
        "\n",
        "Na√Øve Bayes on Synthetic Text Dataset (20 Newsgroups)\n",
        "----------------------------------------------------\n",
        "ROC-AUC Score: 0.9895\n"
      ],
      "metadata": {
        "id": "zt8EQcPgPwQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10:  Imagine you‚Äôre working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "‚óè Text with diverse vocabulary\n",
        "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "‚óè Address class imbalance\n",
        "‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Data Preprocessing\n",
        "\n",
        "a) Handling Missing Data\n",
        "\n",
        "Emails with missing text: Replace missing content with an empty string (\"\") or remove if too incomplete.\n",
        "\n",
        "Missing labels: Remove affected rows to avoid training noise.\n",
        "\n",
        "b) Text Cleaning\n",
        "\n",
        "Convert to lowercase (to ensure \"Free\" and \"free\" are treated the same).\n",
        "\n",
        "Remove punctuation, numbers, and special characters.\n",
        "\n",
        "Remove stopwords (‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù) to reduce noise.\n",
        "\n",
        "Apply stemming/lemmatization (e.g., \"running\" ‚Üí \"run\") to normalize words.\n",
        "\n",
        "c) Text Vectorization\n",
        "\n",
        "Use TF-IDF Vectorization instead of simple bag-of-words for better weighting of important terms.\n",
        "\n",
        "Limit vocabulary size and set max_df and min_df to remove overly common/rare words.\n",
        "\n",
        "2. Choosing & Justifying the Model (4 marks)\n",
        "\n",
        "Na√Øve Bayes:\n",
        "\n",
        "Works extremely well for text classification.\n",
        "\n",
        "Assumes feature independence ‚Äî not always true, but surprisingly effective.\n",
        "\n",
        "Very fast to train and works well with sparse data from TF-IDF.\n",
        "\n",
        "SVM:\n",
        "\n",
        "High accuracy for high-dimensional data like text.\n",
        "\n",
        "Handles non-linear decision boundaries with kernels.\n",
        "\n",
        "More computationally expensive than Na√Øve Bayes.\n",
        "\n",
        "Choice:\n",
        "\n",
        "For speed and scalability ‚Üí Multinomial Na√Øve Bayes.\n",
        "\n",
        "If accuracy is the absolute priority and computation time is acceptable ‚Üí SVM with linear kernel.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "\n",
        "Resampling:\n",
        "\n",
        "Oversample minority class (spam) using SMOTE.\n",
        "\n",
        "Or undersample majority class (not spam) if dataset is very large.\n",
        "\n",
        "Class weights:\n",
        "\n",
        "Use class_weight='balanced' in SVM or adjust priors in Na√Øve Bayes to give spam more influence.\n",
        "\n",
        "Threshold tuning:\n",
        "\n",
        "Adjust decision threshold based on ROC curve to improve recall for spam detection.\n",
        "\n",
        "4. Performance Evaluation\n",
        "\n",
        "Metrics:\n",
        "\n",
        "Precision: Proportion of emails classified as spam that are actually spam (reduces false positives).\n",
        "\n",
        "Recall: Proportion of actual spam emails correctly identified (reduces false negatives).\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC: Measures the trade-off between true positive and false positive rates.\n",
        "\n",
        "Why not just Accuracy?\n",
        "\n",
        "In imbalanced datasets, accuracy can be misleading (e.g., predicting all emails as ‚Äúnot spam‚Äù still gives high accuracy).\n",
        "\n",
        "5. Business Impact (2 marks)\n",
        "\n",
        "Reduces risk: Prevents spam emails from reaching employees or customers, lowering phishing and fraud exposure.\n",
        "\n",
        "Improves productivity: Less time wasted sorting through junk mail.\n",
        "\n",
        "Enhances customer trust: Legitimate emails reach inboxes without being mistakenly flagged as spam.\n",
        "\n",
        "Cost savings: Automated spam filtering reduces manual intervention and IT overhead.\n",
        "\n",
        "‚úÖ Final Summary:\n",
        "For spam classification, I would clean and vectorize text with TF-IDF, handle imbalance via resampling or class weighting, and choose Na√Øve Bayes for speed or SVM for maximum accuracy. I would evaluate with precision, recall, F1, and ROC-AUC to ensure a reliable, business-impactful solution."
      ],
      "metadata": {
        "id": "geVSL6vXQGtC"
      }
    }
  ]
}